---
title: "ST558 Project2"
author: "Shan Luo, Chengxi Zou"
date: '2022-07-03'
params:
  Channels: 'data_channel_is_bus'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(ggplot2)
library(tibble)
```

# Introduction

We are trying to predict the number of shares in social networks using predictive models among different data channels. The data set we have is the Online News Popularity Data Set, which summarizes a heterogeneous set of features about articles published by Mashable in two years.

Since the number of shares is our target variable, the dependent variable in models would be **shares**.

Intuitively, the *number of words in the content*, *number of images*, *number of videos*, *rate of positive words in the content*, *text subjectivity* and *whether published on the weekend or not* would affect the number of shares, so we choose **n_tokens_content**, **num_imgs**, **num_videos**, **global_rate_positive_words**, **global_subjectivity** and **is_weekend** to be the independent variables.

We also take the article categories into our consideration: the effects of variables mentioned above may vary on different types of articles. In order to study that, we could subset the data into each levels of article types before modeling. There exists six types in our data set: lifestyle, entertainment, bus, socmed, tech and world.

As we want to investigate the effect of these independent variables on the number of shares, the first step would be presenting a brief EDA to have a preliminary understanding of data, then modeling data with predictive models and compare them.

The predictive models we choose are linear regression model and ensemble tree-based models, which we'll describe more later.

# Data  

```{r read in and subset data, echo=TRUE, eval=TRUE}
# Read in data and subset data
News <- read_csv("OnlineNewsPopularity.csv", show_col_types = FALSE) 
News <- News %>% 
  filter(!!rlang::sym(params$Channels) == 1) %>%
  select(shares, n_tokens_content, num_imgs, num_videos,
         global_rate_positive_words, global_subjectivity, is_weekend)

# convert the is_weekend variable to a factor
News$is_weekend <- factor(News$is_weekend)
News
```

```{r split train and test data, echo=TRUE, eval=TRUE}
# Split train and test data
set.seed(1)
trainIndex <- createDataPartition(News$shares, p = 0.7, list = FALSE)
train <- News[trainIndex, ]
test <- News[-trainIndex, ]
```

```{r check correlation, echo=TRUE, eval=TRUE}
# Check correlation of all interested variables
cor(select(News ,shares, n_tokens_content, num_imgs, num_videos,
           global_rate_positive_words, global_subjectivity))
```

If two variables have high correlation, we may think about removing one of them.  

# EDA  

## Numeric Summary Table  

```{r numeric summaries, echo=TRUE, eval=TRUE}
# Compute the summary statistics
apply(X = select(train, shares:global_subjectivity), MARGIN = 2,
      FUN = function(x) {
        summaries <- c(min(x), mean(x), median(x), max(x), sd(x), IQR(x))
        names(summaries) <- c("Min", "Mean", "Median", "Max", "Sd", "IQR")
        summaries
      })
```

From numeric summary table, if one variable's mean is greater than median, it has a right skewed distribution. If the mean is less than median, it has a left skewed distribution. If mean is close to median, it may have a symmetric distribution. If the standard deviation is unusual, there may be some outliers.  

## Contingency Table  

```{r contingency table, echo=TRUE, eval=TRUE}
# Create contingency table of predictor "is_weekend"
table(train$is_weekend)
```

From the contingency table, we can see how many articles are published on weekday and weekend.  

```{r contingency table2, echo=TRUE, eval=TRUE}
# Create contingency table of predictor "num_videos"
table(train$num_videos)
```

From the contingency table, we can see the number of articles with different amount of videos.

## Bar Plot  

```{r barplot, echo=TRUE, eval=TRUE}
# Create bar plot of predictor "is_weekend"
g <- ggplot(data = train, aes(x = is_weekend))
g + geom_bar(fill = "cyan2") +
  labs(title = "Bar Plot of is_weekend")
```

From the bar plot, we can see how many articles are published on weekday and weekend and visualize the difference.  

## Histogram  

```{r histograms, echo=TRUE, eval=TRUE}
# Create histogram of response "shares" and fill with predictor "is_weekend"
g <- ggplot(data = train, aes(x = shares))
g + geom_histogram(bins = 30, aes(fill = is_weekend)) +
  labs(x = "Number of Shares",
       title = "Histogram of Shares") +
  scale_fill_discrete(name = "Weekend Published", labels = c("No", "Yes"))
```

For histogram, we can see the distribution of the number of shares. If we have majority of count on the left side and less count on right side, it may have a right skewed distribution. It indicates that most of articles have small number of shares. If we have majority of count on the right side and less count on left side, it may have a left skewed distribution. It indicates that most of articles have large number of shares. If we see a bell shape, it may have a symmetric distribution. It indicates that most of articles have relatively large shares.  

```{r histograms2, echo=TRUE, eval=TRUE}
# Create histogram of response "num_videos" and fill with predictor "is_weekend"
g <- ggplot(data = train, aes(x = num_videos))
g + geom_histogram(bins = 30, aes(fill = is_weekend)) +
  labs(x = "Number of Videos",
       title = "Histogram of Number of Videos") +
  scale_fill_discrete(name = "Weekend Published", labels = c("No", "Yes"))
```

For histogram, we can see the distribution of the number of videos. If we have majority of count on the left side and less count on right side, it may have a right skewed distribution. It indicates that most of articles have small number of videos. If we have majority of count on the right side and less count on left side, it may have a left skewed distribution. It indicates that most of articles have large number of videos. If we see a bell shape, it may have a symmetric distribution. It indicates that the number of videos are approximately normally distributed. 

## Jitter Plot  

```{r jtplot, echo=TRUE, message=FALSE, warning=FALSE}
ggplot(train, aes(x = is_weekend, y = shares)) +
geom_point(aes(color = is_weekend), position = "jitter") + scale_color_discrete(name = "is_weekend") +
  ggtitle("Jitter Plot of shares in weekend/non-weekend") + xlab("is_weekend")
```

We can generate a jitter plot showing the spread of shares data among weekend days and non-weekend days.

## Scatter Plot  

```{r scatterplot, echo=TRUE, eval=TRUE}
# Create scatter plot of response "shares" and predictor "n_tokens_content".
# Filled with predictor "is_weekend"
g <- ggplot(data = train, aes(x = n_tokens_content, y = shares))
g + geom_point(aes(color = is_weekend)) +
  geom_smooth(method = "lm") +
  labs(x = "Number of Words in Content",
       y = "Number of Shares",
       title = "Scatter Plot of Shares vs Number of Words in Content") +
  scale_color_discrete(name = "Weekend Published", labels = c("No", "Yes"))
```

We can inspect the trend of shares as a function of the number of words in content. If the points show an upward trend, then articles with more number of words in the content to be shared more often. If we see a negative trend then articles with more number of words in the content tend to be shared less often.

```{r scatterplot2, echo=TRUE, eval=TRUE}
# Create scatter plot of response "shares" and predictor "num_imgs".
# Filled with predictor "is_weekend"
g <- ggplot(data = train, aes(x = num_imgs, y = shares))
g + geom_point(aes(color = is_weekend)) +
  geom_smooth(method = "lm") +
  labs(x = "Number of Images",
       y = "Number of Shares",
       title = "Scatter Plot of Shares vs Number of images") + 
  scale_color_discrete(name = "Weekend Published", labels = c("No", "Yes"))
```

We can also inspect the trend of shares as a function of the number of images. If the points show an upward trend, then articles with more images would be shared more often. If we see a negative trend then articles with more images tend to be shared less often.

# Modeling  

## Linear Regression  

Regression models allow easy prediction of response and inference. Linear regression is that we model a response as a linear function of some predictors. Model fit by minimizing the sum of squared residuals.  

```{r first linear regression, echo=TRUE, eval=TRUE}
# Fit linear model
mod <- lm(shares ~ (n_tokens_content + num_imgs + num_videos + 
                      global_rate_positive_words + global_subjectivity + 
                      is_weekend)^2 ,data =  train)
# Use forward selection to  choose model
forward_mod <- step(mod, direction = "forward")
# Model fit
lmfit1 <- train(shares ~ (n_tokens_content + num_imgs + num_videos + 
                         global_rate_positive_words + global_subjectivity + 
                         is_weekend)^2,
             data = train,
             method = "lm",
             preProcess = c("center", "scale"),
             trControl = trainControl(method = "cv", number = 5))
lmfit1
# Compute the RMSE, Rsquared, and MAE for comparison
lmpred1 <- predict(lmfit1, newdata = test) 
lm1 <- postResample(lmpred1, test$shares)
lm1
```

```{r second linear regression, echo=TRUE, eval=TRUE}
# Model fit
lmfit2 <- train(shares ~ n_tokens_content + num_imgs + num_videos + 
                         global_rate_positive_words + global_subjectivity + 
                         is_weekend,
             data = train,
             method = "lm",
             preProcess = c("center", "scale"),
             trControl = trainControl(method = "cv", number = 5))
lmfit2
# Compute the RMSE, Rsquared, and MAE for comparison
lmpred2 <- predict(lmfit2, newdata = test) 
lm2 <- postResample(lmpred2, test$shares)
lm2
```

## Random Forest  

Since the response is continuous, we choose to use regression tree.  

The bootstrapping is that we resample from data or a fitted model and apply method or estimation to each resample. We see how the model or method behave.  

For Bootstrap Aggregation(Bagged) for regression tree, we create a bootstrap sample, train tree on the sample, repeat B = 1000 times, and average over these predictions as final prediction.  

Random forest follows Bootstrap Aggregation idea. We will create multiple trees from bootstrap samples and average the results. But, we will use a random subset of predictors for each bootstrap tree fit instead of using all predictors. It may make bagged trees predictions more correlated, which can help with reduction of variation.  

```{r random forest, echo=TRUE, eval=TRUE}
# Fit Random Forest Regression Tree
rffit <- train(shares ~ n_tokens_content + num_imgs + num_videos + 
                         global_rate_positive_words + global_subjectivity + 
                         is_weekend, 
               data = train, 
               method = "rf", 
               trControl = trainControl(method = "cv", number = 5), 
               preProcess = c("center", "scale"),
               tuneGrid = data.frame(mtry = 1:6))
rffit
# Compute the RMSE, Rsquared, and MAE for comparison
rfpred <- predict(rffit, newdata = test) 
rf <- postResample(rfpred, test$shares)
rf
```

## Boosted Tree  

Boosting is a way that slowly trains the tree so that the tree do not over fit. For boosting, trees grow sequentially and each subsequent tree is grown on a modified version of original data. Prediction updates as trees grown.  

The process of boosted tree:  
1. Initialized prediction as 0  
2. Find residuals(observed-predicted) 
3. Fit a tree with d splits(d + 1 terminal nodes) treating the residuals as response  
4. Update predictions  
5. Update residuals for new predictions and repeat B times  

```{r boosted tree, echo=TRUE, eval=TRUE}
# Fit Boosted Regression Tree
boostedTfit <- train(shares ~ n_tokens_content + num_imgs + num_videos + 
                         global_rate_positive_words + global_subjectivity + 
                         is_weekend, 
               data = train, 
               method = "gbm", 
               trControl = trainControl(method = "cv", number = 5), 
               preProcess = c("center", "scale"),
               tuneGrid = data.frame(expand.grid(n.trees = c(25,50,100,150,200), 
                                                 interaction.depth = 1:4,
                                                 shrinkage = 0.1,
                                                 n.minobsinnode = 10)),
               verbose = FALSE)
boostedTfit
# Compute the RMSE, Rsquared, and MAE for comparison
boostedpre <- predict(boostedTfit, newdata = test) 
boosted <- postResample(boostedpre, test$shares)
boosted
```

# Model Comparison  

After fitting these different models, we want to declare the best model by comparing their RMSEs: the model with smallest RMSE is the best model.

```{r comparsion of models, echo=TRUE, eval=TRUE}
allRMSE <- tibble(lm1[1], lm2[1], rf[1], boosted[1])
names(allRMSE) <- c("LinearRegression1", "LinearRegression2", "RandomForest", "BoostedTree")
RMSElong <- allRMSE %>%
  pivot_longer(cols = 1:4, names_to = "Model", values_to = "RMSE")
RMSE_sort <- RMSElong %>% 
  arrange(RMSE)
RMSE_sort[1,]
```

The result is the best model and its RMSE.  

# Automation  

```{r automation, echo=TRUE,eval=FALSE}
channels <- c("data_channel_is_lifestyle", "data_channel_is_entertainment", "data_channel_is_bus", "data_channel_is_socmed", "data_channel_is_tech", "data_channel_is_world")
# Create file names
name <- c("Lifestyle", "Entertainment", "Business", "SocialMedia",
          "Tech", "World")
output_file <- paste0(name, "Analysis.md")
# Create a list for each channel with just channel name parameter
params <- lapply(channels, FUN = function(x){
  list(Channels = x)
})
# Put into a data frame
reports <- tibble::tibble(output_file, params)
# Automation
apply(reports, MARGIN = 1, FUN = function(x) {
  rmarkdown::render(input = "ST558_Project2_Group10.Rmd", 
                    output_format = "github_document", 
                    output_file = x[[1]], 
                    params = x[[2]], 
                    output_options = list(html_preview = FALSE)) 
})
```

# Render Code for Single Channel  

```{r render, echo=TRUE, eval=FALSE}
rmarkdown::render(input = "ST558_Project2_Group10.Rmd", 
                  output_file = "BusinessAnalysis.md", 
                  output_format = "github_document",
                  params = list(Channels = 'data_channel_is_bus'))
```
